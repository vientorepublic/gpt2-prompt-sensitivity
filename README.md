## 다양한 프롬프트에 대한 GPT-2 문장 생성 실험

이 저장소는 GPT-2 언어모델을 활용하여 다양한 프롬프트(빈 문자열, 공백, 마침표 등)에 대해 어떤 문장이 생성되는지 실험하는 코드를 제공합니다.

### 주요 내용

- Hugging Face Transformers 라이브러리 기반 GPT-2 모델 사용
- 여러 형태의 프롬프트(빈 문자열, 공백, 특수문자 등)에 대해 문장 생성
- 각 프롬프트별로 생성된 문장을 CLI에서 보기 쉽도록 출력

### 실행 방법

1. Python 3.8 이상, torch, transformers 설치
2. `run.py` 실행
   ```bash
   python run.py
   ```

### 코드 설명

- `run.py`는 다양한 프롬프트 리스트를 순회하며, 각 프롬프트에 대해 GPT-2로 30개 토큰 분량의 문장을 생성합니다.

### 연구 목적

프롬프트의 형태(빈 문자열, 공백, 마침표 등)가 GPT-2의 문장 생성 결과에 미치는 영향을 관찰하고, 언어모델의 입력 민감성을 실험적으로 분석하는 데 목적이 있습니다.
